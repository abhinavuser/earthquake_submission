{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "introduction"
      },
      "source": [
        "# Enhanced Earthquake Prediction System using Deep Learning\n",
        "\n",
        "This notebook implements an advanced earthquake prediction system using:\n",
        "- Bidirectional LSTM with Attention\n",
        "- Multi-layer Architecture\n",
        "- Real-time USGS Data Integration\n",
        "\n",
        "**Author:** abhinavuser  \n",
        "**Date:** 2025-05-09"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import LayerNorm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "from io import StringIO\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Print versions and device info\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Set random seeds\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dataset"
      },
      "source": [
        "class EarthquakeDataset(Dataset):\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.targets[idx]\n",
        "\n",
        "class CustomRandomSampler(torch.utils.data.Sampler):\n",
        "    def __init__(self, data_source):\n",
        "        self.data_source = data_source\n",
        "        \n",
        "    def __iter__(self):\n",
        "        indices = torch.randperm(len(self.data_source)).tolist()\n",
        "        return iter(indices)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data_source)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "model"
      },
      "source": [
        "class EnhancedEarthquakePredictionModel(nn.Module):\n",
        "    def __init__(self, input_size=10, hidden_size=128, num_layers=3, output_size=3):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        # Input projection\n",
        "        self.input_proj = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            LayerNorm(hidden_size)\n",
        "        )\n",
        "        \n",
        "        # Bidirectional LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=0.3\n",
        "        )\n",
        "        \n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "        \n",
        "        # Output layers\n",
        "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
        "        self.fc3 = nn.Linear(hidden_size // 2, output_size)\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.layer_norm1 = LayerNorm(hidden_size)\n",
        "        self.layer_norm2 = LayerNorm(hidden_size // 2)\n",
        "    \n",
        "    def attention_net(self, lstm_output):\n",
        "        attention_weights = self.attention(lstm_output)\n",
        "        attention_weights = F.softmax(attention_weights, dim=1)\n",
        "        context = torch.sum(attention_weights * lstm_output, dim=1)\n",
        "        return context\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Input projection\n",
        "        x = self.input_proj(x)\n",
        "        \n",
        "        # LSTM\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        \n",
        "        # Attention\n",
        "        context = self.attention_net(lstm_out)\n",
        "        \n",
        "        # Multi-layer prediction\n",
        "        out = self.fc1(context)\n",
        "        out = self.layer_norm1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        \n",
        "        residual = out\n",
        "        out = self.fc2(out)\n",
        "        out = self.layer_norm2(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        \n",
        "        predictions = self.fc3(out)\n",
        "        return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "data_processor"
      },
      "source": [
        "class EarthquakeDataProcessor:\n",
        "    def __init__(self, start_date=None, end_date=None):\n",
        "        self.start_date = start_date or (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')\n",
        "        self.end_date = end_date or datetime.now().strftime('%Y-%m-%d')\n",
        "        self.base_url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
        "    \n",
        "    def load_data(self):\n",
        "        \"\"\"Load earthquake data from USGS API\"\"\"\n",
        "        params = {\n",
        "            'format': 'csv',\n",
        "            'starttime': self.start_date,\n",
        "            'endtime': self.end_date,\n",
        "            'minmagnitude': 2.5\n",
        "        }\n",
        "        \n",
        "        response = requests.get(self.base_url, params=params)\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            self.df = pd.read_csv(StringIO(response.text))\n",
        "            print(f'Loaded {len(self.df)} earthquake records')\n",
        "            return self.df\n",
        "        else:\n",
        "            raise Exception(f\"Failed to fetch data: Status code {response.status_code}\")\n",
        "    \n",
        "    def preprocess_data(self):\n",
        "        \"\"\"Preprocess earthquake data\"\"\"\n",
        "        self.df['datetime'] = pd.to_datetime(self.df['time'])\n",
        "        \n",
        "        # Extract temporal features\n",
        "        self.df['hour'] = self.df['datetime'].dt.hour\n",
        "        self.df['day'] = self.df['datetime'].dt.day\n",
        "        self.df['month'] = self.df['datetime'].dt.month\n",
        "        self.df['year'] = self.df['datetime'].dt.year\n",
        "        self.df['dayofweek'] = self.df['datetime'].dt.dayofweek\n",
        "        \n",
        "        # Calculate additional features\n",
        "        self.df['depth_km'] = self.df['depth'].abs()\n",
        "        self.df['energy'] = 10 ** (1.5 * self.df['mag'])\n",
        "        \n",
        "        # Normalize features\n",
        "        features = ['latitude', 'longitude', 'depth_km', 'mag', 'energy']\n",
        "        self.scaler = StandardScaler()\n",
        "        self.df[features] = self.scaler.fit_transform(self.df[features])\n",
        "        \n",
        "        return self.df\n",
        "    \n",
        "    def create_sequences(self, seq_length=24):\n",
        "        \"\"\"Create sequences for time series prediction\"\"\"\n",
        "        features = ['latitude', 'longitude', 'depth_km', 'mag', 'energy',\n",
        "                   'hour', 'day', 'month', 'year', 'dayofweek']\n",
        "        \n",
        "        sequences = []\n",
        "        targets = []\n",
        "        \n",
        "        for i in range(len(self.df) - seq_length):\n",
        "            seq = self.df[features].iloc[i:i+seq_length].values\n",
        "            target = self.df[['latitude', 'longitude', 'mag']].iloc[i+seq_length].values\n",
        "            sequences.append(seq)\n",
        "            targets.append(target)\n",
        "        \n",
        "        return np.array(sequences, dtype=np.float32), np.array(targets, dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "training"
      },
      "source": [
        "def train_model(model, train_loader, val_loader, epochs=100, learning_rate=0.001):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=learning_rate,\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        pct_start=0.3\n",
        "    )\n",
        "    \n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "    patience_counter = 0\n",
        "    \n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            train_loss += loss.item()\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                val_loss += criterion(output, target).item()\n",
        "        \n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "        \n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        \n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        \n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch+1}')\n",
        "            break\n",
        "        \n",
        "        if epoch % 5 == 0:\n",
        "            print(f'Epoch: {epoch+1}/{epochs}')\n",
        "            print(f'Training Loss: {train_loss:.6f}')\n",
        "            print(f'Validation Loss: {val_loss:.6f}')\n",
        "            print('-' * 50)\n",
        "    \n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    return model, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "visualization"
      },
      "source": [
        "def plot_training_history(history):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['train_loss'], label='Training Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss Over Time')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "main"
      },
      "source": [
        "def main():\n",
        "    try:\n",
        "        # Initialize data processor and load data\n",
        "        processor = EarthquakeDataProcessor()\n",
        "        data = processor.load_data()\n",
        "        processed_data = processor.preprocess_data()\n",
        "        \n",
        "        # Create sequences\n",
        "        sequences, targets = processor.create_sequences(seq_length=24)\n",
        "        \n        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            sequences, targets, test_size=0.2, random_state=42\n",
        "        )\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train, y_train, test_size=0.2, random_state=42\n",
        "        )\n",
        "        \n",
        "        print(f\"Training set shape: {X_train.shape}\")\n",
        "        print(f\"Validation set shape: {X_val.shape}\")\n",
        "        print(f\"Test set shape: {X_test.shape}\")\n",
        "        \n",
        "        # Create datasets and dataloaders\n",
        "        batch_size = min(32, len(X_train) // 10)\n",
        "        train_dataset = EarthquakeDataset(X_train, y_train)\n",
        "        val_dataset = EarthquakeDataset(X_val, y_val)\n",
        "        test_dataset = EarthquakeDataset(X_test, y_test)\n",
        "        \n",
        "        train_sampler = CustomRandomSampler(train_dataset)\n",
        "        \n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, \n",
        "            batch_size=batch_size,\n",
        "            sampler=train_sampler,\n",
        "            drop_last=False\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset, \n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            drop_last=False\n",
        "        )\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset, \n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            drop_last=False\n",
        "        )\n",
        "        \n",
        "        # Initialize model\n",
        "        model = EnhancedEarthquakePredictionModel(\n",
        "            input_size=X_train.shape[2],\n",
        "            hidden_size=128,\n",
        "            num_layers=3,\n",
        "            output_size=3\n",
        "        )\n",
        "        \n        # Train model\n",
        "        trained_model, history = train_model(\n",
        "            model,\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            epochs=100,\n",
        "            learning_rate=0.001\n",
        "        )\n",
        "        \n        print('Training complete!')\n",
        "        \n        # Plot training history\n",
        "        plot_training_history(history)\n",
        "        \n        # Evaluate on test set\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        criterion = nn.MSELoss()\n",
        "        \n        predictions = []\n",
        "        actuals = []\n",
        "        \n        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                test_loss += criterion(output, target).item()\n",
        "                \n",
        "                predictions.extend(output.cpu().numpy())\n",
        "                actuals.extend(target.cpu().numpy())\n",
        "        \n        test_loss /= len(test_loader)\n",
        "        print(f'Final Test Loss: {test_loss:.6f}')\n",
        "        \n        # Save predictions\n",
        "        predictions = np.array(predictions)\n",
        "        actuals = np.array(actuals)\n",
        "        np.save('predictions.npy', predictions)\n",
        "        np.save('actuals.npy', actuals)\n",
        "        \n        # Save model\n",
        "        torch.save({\n",
        "            'model_state_dict': trained_model.state_dict(),\n",
        "            'scaler': processor.scaler,\n",
        "            'history': history\n",
        "        }, 'final_model.pth')\n",
        "        \n        print('Model and predictions saved successfully!')\n",
        "        \n    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n        if __name__ == '__main__':\n",
        "            main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Earthquake_Prediction_Enhanced.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}