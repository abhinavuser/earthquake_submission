{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "introduction"
      },
      "source": [
        "# Earthquake Prediction using Swarm AI and Multi-Agent Reinforcement Learning\n",
        "\n",
        "This notebook implements a sophisticated earthquake prediction system using:\n",
        "- Multi-Agent Reinforcement Learning (MARL)\n",
        "- Swarm Intelligence Algorithms\n",
        "- Deep Learning for time series prediction\n",
        "\n",
        "Author: abhinavuser\n",
        "Date: 2025-05-09"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "# Install required packages\n",
        "!pip install -q pettingzoo==1.24.1\n",
        "!pip install -q stable-baselines3==2.1.0\n",
        "!pip install -q geopandas==0.14.1 folium==0.15.1\n",
        "!pip install -q torch==2.1.0 torchvision==0.16.0\n",
        "!pip install -q gymnasium==0.29.1\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import folium\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import gymnasium as gym\n",
        "from pettingzoo import ParallelEnv\n",
        "from pettingzoo.utils import parallel_to_aec\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Enable GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "data_processor"
      },
      "source": [
        "class EarthquakeDataProcessor:\n",
        "    def __init__(self, start_date=None, end_date=None):\n",
        "        self.start_date = start_date or (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')\n",
        "        self.end_date = end_date or datetime.now().strftime('%Y-%m-%d')\n",
        "        self.base_url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
        "        \n",
        "    def load_data(self):\n",
        "        \"\"\"Load earthquake data from USGS API\"\"\"\n",
        "        params = {\n",
        "            'format': 'csv',\n",
        "            'starttime': self.start_date,\n",
        "            'endtime': self.end_date,\n",
        "            'minmagnitude': 2.5\n",
        "        }\n",
        "        \n",
        "        self.df = pd.read_csv(self.base_url, params=params)\n",
        "        print(f'Loaded {len(self.df)} earthquake records')\n",
        "        return self.df\n",
        "    \n",
        "    def preprocess_data(self):\n",
        "        \"\"\"Preprocess earthquake data for model input\"\"\"\n",
        "        # Convert time to datetime\n",
        "        self.df['datetime'] = pd.to_datetime(self.df['time'])\n",
        "        \n",
        "        # Extract temporal features\n",
        "        self.df['hour'] = self.df['datetime'].dt.hour\n",
        "        self.df['day'] = self.df['datetime'].dt.day\n",
        "        self.df['month'] = self.df['datetime'].dt.month\n",
        "        self.df['year'] = self.df['datetime'].dt.year\n",
        "        self.df['dayofweek'] = self.df['datetime'].dt.dayofweek\n",
        "        \n        # Calculate additional features\n",
        "        self.df['depth_km'] = self.df['depth'].abs()\n",
        "        self.df['energy'] = 10 ** (1.5 * self.df['mag'])\n",
        "        \n        # Normalize features\n",
        "        features = ['latitude', 'longitude', 'depth_km', 'mag', 'energy']\n",
        "        self.scaler = StandardScaler()\n",
        "        self.df[features] = self.scaler.fit_transform(self.df[features])\n",
        "        \n        return self.df\n",
        "\n        def create_sequences(self, seq_length=24):\n",
        "            \"\"\"Create sequences for time series prediction\"\"\"\n",
        "            features = ['latitude', 'longitude', 'depth_km', 'mag', 'energy',\n",
        "                       'hour', 'day', 'month', 'year', 'dayofweek']\n",
        "            \n            sequences = []\n",
        "            targets = []\n",
        "            \n            for i in range(len(self.df) - seq_length):\n",
        "                seq = self.df[features].iloc[i:i+seq_length].values\n",
        "                target = self.df[['latitude', 'longitude', 'mag']].iloc[i+seq_length].values\n",
        "                sequences.append(seq)\n",
        "                targets.append(target)\n",
        "            \n            return np.array(sequences), np.array(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "earthquake_environment"
      },
      "source": [
        "class EarthquakeEnvironment(ParallelEnv):\n",
        "    def __init__(self, num_agents=5, grid_size=50, max_steps=1000):\n",
        "        super().__init__()\n",
        "        self.num_agents = num_agents\n",
        "        self.grid_size = grid_size\n",
        "        self.max_steps = max_steps\n",
        "        self.current_step = 0\n",
        "        \n        # Define agents\n",
        "        self.possible_agents = [f'agent_{i}' for i in range(num_agents)]\n",
        "        self.agents = self.possible_agents.copy()\n",
        "        \n        # Define observation and action spaces\n",
        "        self.observation_spaces = {agent: gym.spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(10,), dtype=np.float32\n",
        "        ) for agent in self.possible_agents}\n",
        "        \n        self.action_spaces = {agent: gym.spaces.Box(\n",
        "            low=-1, high=1, shape=(4,), dtype=np.float32\n",
        "        ) for agent in self.possible_agents}\n",
        "    \n        def reset(self, seed=None, options=None):\n",
        "            \"\"\"Reset environment to initial state\"\"\"\n",
        "            self.agents = self.possible_agents.copy()\n",
        "            self.current_step = 0\n",
        "            \n            observations = {agent: self._get_observation() \n",
        "                                        for agent in self.agents}\n",
        "            infos = {agent: {} for agent in self.agents}\n",
        "            \n            return observations, infos\n",
        "        \n        def step(self, actions):\n",
        "            \"\"\"Execute environment step\"\"\"\n",
        "            # Execute actions and get rewards\n",
        "            rewards = {agent: self._calculate_reward(actions[agent]) \n",
        "                      for agent in self.agents}\n",
        "            \n            # Get new observations\n",
        "            observations = {agent: self._get_observation() \n",
        "                          for agent in self.agents}\n",
        "            \n            # Check termination conditions\n",
        "            self.current_step += 1\n",
        "            terminated = {agent: self.current_step >= self.max_steps \n",
        "                         for agent in self.agents}\n",
        "            truncated = {agent: False for agent in self.agents}\n",
        "            infos = {agent: {} for agent in self.agents}\n",
        "            \n            return observations, rewards, terminated, truncated, infos\n",
        "        \n        def _get_observation(self):\n",
        "            \"\"\"Generate observation for an agent\"\"\"\n",
        "            return np.random.normal(0, 1, (10,)).astype(np.float32)\n",
        "        \n        def _calculate_reward(self, action):\n",
        "            \"\"\"Calculate reward for an action\"\"\"\n",
        "            # Implement your reward function here\n",
        "            return float(np.sum(action ** 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swarm_optimization"
      },
      "source": [
        "class ParticleSwarmOptimization:\n",
        "    def __init__(self, num_particles, dimensions, bounds, w=0.7, c1=1.5, c2=1.5):\n",
        "        self.num_particles = num_particles\n",
        "        self.dimensions = dimensions\n",
        "        self.bounds = bounds\n",
        "        self.w = w  # inertia weight\n",
        "        self.c1 = c1  # cognitive weight\n",
        "        self.c2 = c2  # social weight\n",
        "        \n        # Initialize particles\n",
        "        self.particles = self._initialize_particles()\n",
        "        self.global_best_position = None\n",
        "        self.global_best_score = float('inf')\n",
        "    \n        def _initialize_particles(self):\n",
        "            \"\"\"Initialize particle positions and velocities\"\"\"\n",
        "            particles = []\n",
        "            for _ in range(self.num_particles):\n",
        "                position = np.random.uniform(\n",
        "                    self.bounds[0], self.bounds[1], self.dimensions\n",
        "                )\n",
        "                velocity = np.zeros(self.dimensions)\n",
        "                particles.append({\n",
        "                    'position': position,\n",
        "                    'velocity': velocity,\n",
        "                    'best_position': position.copy(),\n",
        "                    'best_score': float('inf')\n",
        "                })\n",
        "            return particles\n",
        "        \n        def optimize(self, objective_function, max_iterations=100):\n",
        "            \"\"\"Run PSO optimization\"\"\"\n",
        "            for iteration in range(max_iterations):\n",
        "                for particle in self.particles:\n",
        "                    # Evaluate current position\n",
        "                    score = objective_function(particle['position'])\n",
        "                    \n                    # Update personal best\n",
        "                    if score < particle['best_score']:\n",
        "                        particle['best_score'] = score\n",
        "                        particle['best_position'] = particle['position'].copy()\n",
        "                    \n                    # Update global best\n",
        "                    if score < self.global_best_score:\n",
        "                        self.global_best_score = score\n",
        "                        self.global_best_position = particle['position'].copy()\n",
        "                    \n                    # Update velocity and position\n",
        "                    r1, r2 = np.random.rand(2)\n",
        "                    particle['velocity'] = (\n",
        "                        self.w * particle['velocity'] +\n",
        "                        self.c1 * r1 * (particle['best_position'] - particle['position']) +\n",
        "                        self.c2 * r2 * (self.global_best_position - particle['position'])\n",
        "                    )\n",
        "                    \n                    # Update position\n",
        "                    particle['position'] += particle['velocity']\n",
        "                    particle['position'] = np.clip(\n",
        "                        particle['position'], self.bounds[0], self.bounds[1]\n",
        "                    )\n",
        "                \n                if iteration % 10 == 0:\n",
        "                    print(f'Iteration {iteration}, Best Score: {self.global_best_score:.6f}')\n",
        "            \n            return self.global_best_position, self.global_best_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neural_network"
      },
      "source": [
        "class EarthquakePredictionModel(nn.Module):\n",
        "    def __init__(self, input_size=10, hidden_size=64, num_layers=2, output_size=3):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0.2\n",
        "        )\n",
        "        \n        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size // 2, output_size)\n",
        "        )\n",
        "    \n        def forward(self, x):\n",
        "            lstm_out, _ = self.lstm(x)\n",
        "            predictions = self.fc(lstm_out[:, -1, :])\n",
        "            return predictions\n",
        "\n        class EarthquakeDataset(Dataset):\n",
        "            def __init__(self, sequences, targets):\n",
        "                self.sequences = torch.FloatTensor(sequences)\n",
        "                self.targets = torch.FloatTensor(targets)\n",
        "            \n            def __len__(self):\n",
        "                return len(self.sequences)\n",
        "            \n            def __getitem__(self, idx):\n",
        "                return self.sequences[idx], self.targets[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "training"
      },
      "source": [
        "def train_model(model, train_loader, val_loader, epochs=100, learning_rate=0.001):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
        "    \n    best_val_loss = float('inf')\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        \n        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                val_loss += criterion(output, target).item()\n",
        "        \n        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "        scheduler.step(val_loss)\n",
        "        \n        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "        \n        print(f'Epoch: {epoch+1}/{epochs}')\n",
        "        print(f'Training Loss: {train_loss:.6f}')\n",
        "        print(f'Validation Loss: {val_loss:.6f}')\n",
        "        print('-' * 50)\n",
        "    \n    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "main_execution"
      },
      "source": [
        "def main():\n",
        "    # Initialize data processor and load data\n",
        "    processor = EarthquakeDataProcessor()\n",
        "    data = processor.load_data()\n",
        "    processed_data = processor.preprocess_data()\n",
        "    \n    # Create sequences for training\n",
        "    sequences, targets = processor.create_sequences(seq_length=24)\n",
        "    \n    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        sequences, targets, test_size=0.2, random_state=42\n",
        "    )\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train, y_train, test_size=0.2, random_state=42\n",
        "    )\n",
        "    \n    # Create datasets and dataloaders\n",
        "    train_dataset = EarthquakeDataset(X_train, y_train)\n",
        "    val_dataset = EarthquakeDataset(X_val, y_val)\n",
        "    test_dataset = EarthquakeDataset(X_test, y_test)\n",
        "    \n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "    \n    # Initialize model\n",
        "    model = EarthquakePredictionModel(\n",
        "        input_size=10,\n",
        "        hidden_size=64,\n",
        "        num_layers=2,\n",
        "        output_size=3\n",
        "    )\n",
        "    \n    # Train model\n",
        "    trained_model = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        epochs=100,\n",
        "        learning_rate=0.001\n",
        "    )\n",
        "    \n    # Initialize MARL environment\n",
        "    env = EarthquakeEnvironment(num_agents=5)\n",
        "    \n    # Initialize PSO optimizer\n",
        "    pso = ParticleSwarmOptimization(\n",
        "        num_particles=20,\n",
        "        dimensions=model.count_parameters(),\n",
        "        bounds=(-1, 1)\n",
        "    )\n",
        "    \n    print('Training complete!')\n",
        "\n        if __name__ == '__main__':\n",
        "            main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Earthquake_Prediction_MARL_Swarm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}